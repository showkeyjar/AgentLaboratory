"""
æ•°æ®æ ¼å¼æ£€æµ‹å™¨æ¼”ç¤ºè„šæœ¬

å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ•°æ®æ ¼å¼æ£€æµ‹å™¨è¿›è¡Œå¤šæ ¼å¼æ•°æ®è¯†åˆ«å’Œè‡ªåŠ¨æ ¼å¼è½¬æ¢
"""

import sys
import os
import json
import csv
import tempfile
import shutil
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from research_automation.core.data_format_detector import (
    DataFormatDetector, DataFormat, DataFormatConfidence
)


def create_sample_data_files(temp_dir):
    """åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶"""
    sample_files = {}
    
    # 1. CSVæ–‡ä»¶ - å­¦ç”Ÿæˆç»©æ•°æ®
    csv_file = os.path.join(temp_dir, "students_scores.csv")
    with open(csv_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['å­¦å·', 'å§“å', 'æ•°å­¦', 'è‹±è¯­', 'ç‰©ç†', 'æ€»åˆ†'])
        writer.writerow(['2021001', 'å¼ ä¸‰', 95, 88, 92, 275])
        writer.writerow(['2021002', 'æå››', 87, 94, 89, 270])
        writer.writerow(['2021003', 'ç‹äº”', 92, 85, 96, 273])
        writer.writerow(['2021004', 'èµµå…­', 89, 91, 88, 268])
        writer.writerow(['2021005', 'é’±ä¸ƒ', 96, 87, 94, 277])
    sample_files['CSVå­¦ç”Ÿæˆç»©'] = csv_file
    
    # 2. JSONæ–‡ä»¶ - äº§å“ä¿¡æ¯
    json_file = os.path.join(temp_dir, "products.json")
    products_data = [
        {
            "id": "P001",
            "name": "æ™ºèƒ½æ‰‹æœº",
            "category": "ç”µå­äº§å“",
            "price": 2999.99,
            "stock": 150,
            "specifications": {
                "screen": "6.1è‹±å¯¸",
                "memory": "128GB",
                "camera": "48MP"
            }
        },
        {
            "id": "P002", 
            "name": "ç¬”è®°æœ¬ç”µè„‘",
            "category": "ç”µå­äº§å“",
            "price": 5999.99,
            "stock": 75,
            "specifications": {
                "cpu": "Intel i7",
                "memory": "16GB",
                "storage": "512GB SSD"
            }
        },
        {
            "id": "P003",
            "name": "æ— çº¿è€³æœº",
            "category": "éŸ³é¢‘è®¾å¤‡", 
            "price": 299.99,
            "stock": 200,
            "specifications": {
                "battery": "24å°æ—¶",
                "connectivity": "è“ç‰™5.0",
                "noise_cancelling": True
            }
        }
    ]
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(products_data, f, ensure_ascii=False, indent=2)
    sample_files['JSONäº§å“ä¿¡æ¯'] = json_file
    
    # 3. XMLæ–‡ä»¶ - å›¾ä¹¦ç›®å½•
    xml_file = os.path.join(temp_dir, "books.xml")
    xml_content = '''<?xml version="1.0" encoding="UTF-8"?>
<library>
    <book id="B001">
        <title>Pythonç¼–ç¨‹ä»å…¥é—¨åˆ°å®è·µ</title>
        <author>Eric Matthes</author>
        <publisher>äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾</publisher>
        <year>2020</year>
        <price>89.00</price>
        <category>ç¼–ç¨‹</category>
    </book>
    <book id="B002">
        <title>æ·±åº¦å­¦ä¹ </title>
        <author>Ian Goodfellow</author>
        <publisher>äººæ°‘é‚®ç”µå‡ºç‰ˆç¤¾</publisher>
        <year>2019</year>
        <price>168.00</price>
        <category>äººå·¥æ™ºèƒ½</category>
    </book>
    <book id="B003">
        <title>ç®—æ³•å¯¼è®º</title>
        <author>Thomas H. Cormen</author>
        <publisher>æœºæ¢°å·¥ä¸šå‡ºç‰ˆç¤¾</publisher>
        <year>2018</year>
        <price>128.00</price>
        <category>ç®—æ³•</category>
    </book>
</library>'''
    with open(xml_file, 'w', encoding='utf-8') as f:
        f.write(xml_content)
    sample_files['XMLå›¾ä¹¦ç›®å½•'] = xml_file
    
    # 4. TSVæ–‡ä»¶ - å®éªŒæ•°æ®
    tsv_file = os.path.join(temp_dir, "experiment_data.tsv")
    with open(tsv_file, 'w', encoding='utf-8') as f:
        f.write("å®éªŒç¼–å·\tæ¸©åº¦\tå‹åŠ›\tæ¹¿åº¦\tç»“æœ\n")
        f.write("EXP001\t25.5\t101.3\t65\tæˆåŠŸ\n")
        f.write("EXP002\t27.2\t102.1\t68\tæˆåŠŸ\n")
        f.write("EXP003\t23.8\t100.8\t62\tå¤±è´¥\n")
        f.write("EXP004\t26.1\t101.7\t66\tæˆåŠŸ\n")
    sample_files['TSVå®éªŒæ•°æ®'] = tsv_file
    
    # 5. JSONLæ–‡ä»¶ - æ—¥å¿—æ•°æ®
    jsonl_file = os.path.join(temp_dir, "logs.jsonl")
    log_entries = [
        {"timestamp": "2024-01-15T10:30:00", "level": "INFO", "message": "ç³»ç»Ÿå¯åŠ¨æˆåŠŸ", "module": "main"},
        {"timestamp": "2024-01-15T10:31:15", "level": "DEBUG", "message": "åŠ è½½é…ç½®æ–‡ä»¶", "module": "config"},
        {"timestamp": "2024-01-15T10:32:30", "level": "WARNING", "message": "å†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜", "module": "monitor"},
        {"timestamp": "2024-01-15T10:33:45", "level": "ERROR", "message": "æ•°æ®åº“è¿æ¥å¤±è´¥", "module": "database"},
        {"timestamp": "2024-01-15T10:34:00", "level": "INFO", "message": "é‡æ–°è¿æ¥æ•°æ®åº“æˆåŠŸ", "module": "database"}
    ]
    with open(jsonl_file, 'w', encoding='utf-8') as f:
        for entry in log_entries:
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    sample_files['JSONLæ—¥å¿—æ•°æ®'] = jsonl_file
    
    # 6. æ–‡æœ¬æ–‡ä»¶ - ç ”ç©¶æŠ¥å‘Š
    text_file = os.path.join(temp_dir, "research_report.txt")
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write("äººå·¥æ™ºèƒ½åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ç ”ç©¶æŠ¥å‘Š\n")
        f.write("=" * 40 + "\n\n")
        f.write("æ‘˜è¦ï¼š\n")
        f.write("æœ¬ç ”ç©¶æ¢è®¨äº†äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨ç°ä»£æ•™è‚²ä¸­çš„åº”ç”¨ç°çŠ¶å’Œå‘å±•è¶‹åŠ¿ã€‚\n")
        f.write("é€šè¿‡åˆ†æå¤§é‡æ¡ˆä¾‹å’Œæ•°æ®ï¼Œæˆ‘ä»¬å‘ç°AIæŠ€æœ¯æ­£åœ¨é©å‘½æ€§åœ°æ”¹å˜æ•™è‚²æ–¹å¼ã€‚\n\n")
        f.write("å…³é”®è¯ï¼šäººå·¥æ™ºèƒ½ã€æ•™è‚²æŠ€æœ¯ã€ä¸ªæ€§åŒ–å­¦ä¹ ã€æ™ºèƒ½è¯„ä¼°\n\n")
        f.write("1. å¼•è¨€\n")
        f.write("éšç€äººå·¥æ™ºèƒ½æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œå…¶åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›...\n\n")
        f.write("2. ç ”ç©¶æ–¹æ³•\n")
        f.write("æœ¬ç ”ç©¶é‡‡ç”¨äº†å®šé‡å’Œå®šæ€§ç›¸ç»“åˆçš„ç ”ç©¶æ–¹æ³•...\n\n")
        f.write("3. ç»“æœä¸åˆ†æ\n")
        f.write("ç ”ç©¶ç»“æœè¡¨æ˜ï¼ŒAIæŠ€æœ¯åœ¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢æ˜¾ç¤ºå‡ºå·¨å¤§æ½œåŠ›...\n")
    sample_files['TXTç ”ç©¶æŠ¥å‘Š'] = text_file
    
    return sample_files


def demonstrate_format_detection(detector, sample_files):
    """æ¼”ç¤ºæ ¼å¼æ£€æµ‹åŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("ğŸ“‹ æ•°æ®æ ¼å¼æ£€æµ‹æ¼”ç¤º")
    print("=" * 80)
    
    detection_results = {}
    
    for file_desc, file_path in sample_files.items():
        print(f"\nğŸ” æ£€æµ‹æ–‡ä»¶: {file_desc}")
        print("-" * 60)
        
        # æ‰§è¡Œæ ¼å¼æ£€æµ‹
        result = detector.detect_format(file_path)
        detection_results[file_desc] = result
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {os.path.basename(file_path)}")
        print(f"ğŸ¯ æ£€æµ‹æ ¼å¼: {result.detected_format.value}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {result.confidence_score:.2%} ({result.confidence.value})")
        print(f"ğŸ”¤ æ–‡ä»¶ç¼–ç : {result.encoding}")
        
        if result.format_features:
            print(f"ğŸ”§ æ ¼å¼ç‰¹å¾:")
            for key, value in result.format_features.items():
                if key == 'delimiter':
                    value = repr(value)  # æ˜¾ç¤ºç‰¹æ®Šå­—ç¬¦
                elif isinstance(value, list) and len(value) > 5:
                    value = value[:5] + ['...']  # æˆªæ–­é•¿åˆ—è¡¨
                print(f"   â€¢ {key}: {value}")
        
        if result.conversion_suggestions:
            print(f"ğŸ’¡ è½¬æ¢å»ºè®®:")
            for i, suggestion in enumerate(result.conversion_suggestions[:3], 1):
                print(f"   {i}. {suggestion}")
        
        if result.detection_errors:
            print(f"âŒ æ£€æµ‹é”™è¯¯:")
            for error in result.detection_errors:
                print(f"   â€¢ {error}")
        
        # æ˜¾ç¤ºå¯é æ€§
        reliability = "âœ… å¯é " if result.is_reliable() else "âš ï¸ éœ€ç¡®è®¤"
        print(f"ğŸ–ï¸ æ£€æµ‹å¯é æ€§: {reliability}")
    
    return detection_results


def demonstrate_batch_detection(detector, sample_files):
    """æ¼”ç¤ºæ‰¹é‡æ£€æµ‹åŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("ğŸ“¦ æ‰¹é‡æ ¼å¼æ£€æµ‹æ¼”ç¤º")
    print("=" * 80)
    
    file_paths = list(sample_files.values())
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡æ£€æµ‹ {len(file_paths)} ä¸ªæ–‡ä»¶...")
    
    # æ‰§è¡Œæ‰¹é‡æ£€æµ‹
    batch_results = detector.batch_detect_formats(file_paths)
    
    # åˆ›å»ºæ£€æµ‹ç»“æœæ±‡æ€»è¡¨
    print(f"\nğŸ“Š æ‰¹é‡æ£€æµ‹ç»“æœæ±‡æ€»:")
    print("-" * 80)
    print(f"{'æ–‡ä»¶å':<25} {'æ ¼å¼':<10} {'ç½®ä¿¡åº¦':<10} {'å¯é æ€§':<10} {'ç¼–ç ':<10}")
    print("-" * 80)
    
    for file_path, result in batch_results.items():
        filename = os.path.basename(file_path)
        format_name = result.detected_format.value
        confidence = f"{result.confidence_score:.1%}"
        reliability = "å¯é " if result.is_reliable() else "éœ€ç¡®è®¤"
        encoding = result.encoding
        
        print(f"{filename:<25} {format_name:<10} {confidence:<10} {reliability:<10} {encoding:<10}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    format_counts = {}
    reliable_count = 0
    
    for result in batch_results.values():
        format_name = result.detected_format.value
        format_counts[format_name] = format_counts.get(format_name, 0) + 1
        if result.is_reliable():
            reliable_count += 1
    
    print("\nğŸ“ˆ æ£€æµ‹ç»Ÿè®¡:")
    print(f"â€¢ æ€»æ–‡ä»¶æ•°: {len(batch_results)}")
    print(f"â€¢ å¯é æ£€æµ‹: {reliable_count}/{len(batch_results)} ({reliable_count/len(batch_results):.1%})")
    print(f"â€¢ æ ¼å¼åˆ†å¸ƒ:")
    for format_name, count in format_counts.items():
        print(f"  - {format_name}: {count} ä¸ªæ–‡ä»¶")


def demonstrate_format_conversion(detector, sample_files, temp_dir):
    """æ¼”ç¤ºæ ¼å¼è½¬æ¢åŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("ğŸ”„ æ•°æ®æ ¼å¼è½¬æ¢æ¼”ç¤º")
    print("=" * 80)
    
    conversion_demos = [
        {
            'name': 'CSV â†’ JSON',
            'source': sample_files['CSVå­¦ç”Ÿæˆç»©'],
            'target_format': DataFormat.JSON,
            'output': os.path.join(temp_dir, 'students_scores.json')
        },
        {
            'name': 'JSON â†’ CSV', 
            'source': sample_files['JSONäº§å“ä¿¡æ¯'],
            'target_format': DataFormat.CSV,
            'output': os.path.join(temp_dir, 'products.csv')
        },
        {
            'name': 'XML â†’ JSON',
            'source': sample_files['XMLå›¾ä¹¦ç›®å½•'],
            'target_format': DataFormat.JSON,
            'output': os.path.join(temp_dir, 'books.json')
        },
        {
            'name': 'TSV â†’ JSON',
            'source': sample_files['TSVå®éªŒæ•°æ®'],
            'target_format': DataFormat.JSON,
            'output': os.path.join(temp_dir, 'experiment_data.json')
        }
    ]
    
    conversion_results = []
    
    for demo in conversion_demos:
        print(f"\nğŸ”„ è½¬æ¢æ¼”ç¤º: {demo['name']}")
        print("-" * 60)
        
        # æ‰§è¡Œæ ¼å¼è½¬æ¢
        result = detector.convert_format(
            demo['source'],
            demo['target_format'],
            demo['output']
        )
        
        conversion_results.append((demo['name'], result))
        
        # æ˜¾ç¤ºè½¬æ¢ç»“æœ
        if result.success:
            print(f"âœ… è½¬æ¢æˆåŠŸ!")
            print(f"ğŸ“ æºæ ¼å¼: {result.source_format.value}")
            print(f"ğŸ¯ ç›®æ ‡æ ¼å¼: {result.target_format.value}")
            print(f"â±ï¸ è½¬æ¢æ—¶é—´: {result.conversion_time_seconds:.3f}ç§’")
            print(f"ğŸ“Š å¤„ç†æ•°æ®: {result.rows_processed} è¡Œ Ã— {result.columns_processed} åˆ—")
            print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {os.path.basename(result.output_path)}")
            
            if result.has_warnings():
                print(f"âš ï¸ è½¬æ¢è­¦å‘Š:")
                for warning in result.conversion_warnings:
                    print(f"   â€¢ {warning}")
            
            # æ˜¾ç¤ºè½¬æ¢åæ•°æ®çš„é¢„è§ˆ
            if hasattr(result.converted_data, '__len__') and len(result.converted_data) > 0:
                print(f"ğŸ‘€ æ•°æ®é¢„è§ˆ:")
                if isinstance(result.converted_data, list):
                    preview_count = min(2, len(result.converted_data))
                    for i in range(preview_count):
                        print(f"   è®°å½•{i+1}: {str(result.converted_data[i])[:100]}...")
                elif isinstance(result.converted_data, dict):
                    keys = list(result.converted_data.keys())[:3]
                    print(f"   ä¸»è¦é”®: {keys}")
        else:
            print(f"âŒ è½¬æ¢å¤±è´¥!")
            if result.has_errors():
                print(f"ğŸš« é”™è¯¯ä¿¡æ¯:")
                for error in result.conversion_errors:
                    print(f"   â€¢ {error}")
    
    # è½¬æ¢ç»“æœæ±‡æ€»
    print(f"\nğŸ“Š è½¬æ¢ç»“æœæ±‡æ€»:")
    print("-" * 60)
    
    successful_conversions = sum(1 for _, result in conversion_results if result.success)
    total_conversions = len(conversion_results)
    
    print(f"â€¢ æ€»è½¬æ¢ä»»åŠ¡: {total_conversions}")
    print(f"â€¢ æˆåŠŸè½¬æ¢: {successful_conversions}")
    print(f"â€¢ æˆåŠŸç‡: {successful_conversions/total_conversions:.1%}")
    
    if successful_conversions > 0:
        avg_time = sum(result.conversion_time_seconds for _, result in conversion_results if result.success) / successful_conversions
        print(f"â€¢ å¹³å‡è½¬æ¢æ—¶é—´: {avg_time:.3f}ç§’")


def demonstrate_data_content_detection(detector):
    """æ¼”ç¤ºä»æ•°æ®å†…å®¹æ£€æµ‹æ ¼å¼"""
    print("\n" + "=" * 80)
    print("ğŸ“ æ•°æ®å†…å®¹æ ¼å¼æ£€æµ‹æ¼”ç¤º")
    print("=" * 80)
    
    # å‡†å¤‡ä¸åŒæ ¼å¼çš„æ•°æ®å†…å®¹
    data_samples = [
        {
            'name': 'CSVæ•°æ®',
            'content': 'name,age,city\nå¼ ä¸‰,25,åŒ—äº¬\næå››,30,ä¸Šæµ·\nç‹äº”,28,å¹¿å·',
            'filename': 'sample.csv'
        },
        {
            'name': 'JSONå¯¹è±¡',
            'content': '{"name": "å¼ ä¸‰", "age": 25, "city": "åŒ—äº¬", "hobbies": ["è¯»ä¹¦", "æ¸¸æ³³"]}',
            'filename': 'sample.json'
        },
        {
            'name': 'JSONæ•°ç»„',
            'content': '[{"id": 1, "name": "äº§å“A"}, {"id": 2, "name": "äº§å“B"}]',
            'filename': 'products.json'
        },
        {
            'name': 'XMLæ–‡æ¡£',
            'content': '<?xml version="1.0"?><root><item>å€¼1</item><item>å€¼2</item></root>',
            'filename': 'data.xml'
        },
        {
            'name': 'çº¯æ–‡æœ¬',
            'content': 'è¿™æ˜¯ä¸€æ®µçº¯æ–‡æœ¬å†…å®¹ã€‚\nåŒ…å«å¤šè¡Œæ–‡å­—ã€‚\nç”¨äºæµ‹è¯•æ–‡æœ¬æ ¼å¼æ£€æµ‹ã€‚',
            'filename': 'text.txt'
        }
    ]
    
    for sample in data_samples:
        print(f"\nğŸ” æ£€æµ‹æ•°æ®: {sample['name']}")
        print("-" * 40)
        
        # æ˜¾ç¤ºæ•°æ®å†…å®¹ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
        content_preview = sample['content'][:100]
        if len(sample['content']) > 100:
            content_preview += "..."
        print(f"ğŸ“„ æ•°æ®å†…å®¹: {content_preview}")
        
        # æ‰§è¡Œæ ¼å¼æ£€æµ‹
        result = detector.detect_format_from_data(sample['content'], sample['filename'])
        
        # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
        print(f"ğŸ¯ æ£€æµ‹æ ¼å¼: {result.detected_format.value}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {result.confidence_score:.2%}")
        print(f"ğŸ”¤ ç¼–ç : {result.encoding}")
        
        if result.format_features:
            print(f"ğŸ”§ æ ¼å¼ç‰¹å¾: {list(result.format_features.keys())}")


def demonstrate_supported_formats(detector):
    """æ¼”ç¤ºæ”¯æŒçš„æ ¼å¼å’Œè½¬æ¢è·¯å¾„"""
    print("\n" + "=" * 80)
    print("ğŸ› ï¸ æ”¯æŒçš„æ ¼å¼å’Œè½¬æ¢è·¯å¾„")
    print("=" * 80)
    
    # æ˜¾ç¤ºæ”¯æŒçš„æ ¼å¼
    supported_formats = detector.get_supported_formats()
    print(f"\nğŸ“‹ æ”¯æŒçš„æ•°æ®æ ¼å¼ ({len(supported_formats)} ç§):")
    for i, data_format in enumerate(supported_formats, 1):
        print(f"  {i}. {data_format.value}")
    
    # æ˜¾ç¤ºæ”¯æŒçš„è½¬æ¢è·¯å¾„
    supported_conversions = detector.get_supported_conversions()
    print(f"\nğŸ”„ æ”¯æŒçš„è½¬æ¢è·¯å¾„ ({len(supported_conversions)} ç§):")
    
    # æŒ‰æºæ ¼å¼åˆ†ç»„æ˜¾ç¤º
    conversion_groups = {}
    for source_format, target_format in supported_conversions:
        if source_format not in conversion_groups:
            conversion_groups[source_format] = []
        conversion_groups[source_format].append(target_format)
    
    for source_format, target_formats in conversion_groups.items():
        targets = ", ".join([tf.value for tf in target_formats])
        print(f"  â€¢ {source_format.value} â†’ {targets}")


def generate_summary_report(detection_results, temp_dir):
    """ç”Ÿæˆæ£€æµ‹å’Œè½¬æ¢æ±‡æ€»æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ğŸ“Š æ±‡æ€»æŠ¥å‘Š")
    print("=" * 80)
    
    # æ£€æµ‹ç»“æœç»Ÿè®¡
    total_files = len(detection_results)
    reliable_detections = sum(1 for result in detection_results.values() if result.is_reliable())
    
    format_distribution = {}
    confidence_levels = {}
    
    for result in detection_results.values():
        # æ ¼å¼åˆ†å¸ƒ
        format_name = result.detected_format.value
        format_distribution[format_name] = format_distribution.get(format_name, 0) + 1
        
        # ç½®ä¿¡åº¦çº§åˆ«åˆ†å¸ƒ
        confidence_level = result.confidence.value
        confidence_levels[confidence_level] = confidence_levels.get(confidence_level, 0) + 1
    
    print(f"\nğŸ“ˆ æ£€æµ‹æ€§èƒ½ç»Ÿè®¡:")
    print(f"â€¢ æ€»æ£€æµ‹æ–‡ä»¶æ•°: {total_files}")
    print(f"â€¢ å¯é æ£€æµ‹æ•°: {reliable_detections}")
    print(f"â€¢ å¯é æ£€æµ‹ç‡: {reliable_detections/total_files:.1%}")
    
    print(f"\nğŸ“Š æ ¼å¼åˆ†å¸ƒ:")
    for format_name, count in sorted(format_distribution.items()):
        percentage = count / total_files * 100
        print(f"â€¢ {format_name}: {count} ä¸ªæ–‡ä»¶ ({percentage:.1f}%)")
    
    print(f"\nğŸ¯ ç½®ä¿¡åº¦åˆ†å¸ƒ:")
    for level, count in sorted(confidence_levels.items()):
        percentage = count / total_files * 100
        print(f"â€¢ {level}: {count} ä¸ªæ–‡ä»¶ ({percentage:.1f}%)")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_file = os.path.join(temp_dir, "detection_report.json")
    report_data = {
        'timestamp': datetime.now().isoformat(),
        'summary': {
            'total_files': total_files,
            'reliable_detections': reliable_detections,
            'reliability_rate': reliable_detections / total_files,
            'format_distribution': format_distribution,
            'confidence_distribution': confidence_levels
        },
        'detailed_results': {
            file_desc: {
                'detected_format': result.detected_format.value,
                'confidence_score': result.confidence_score,
                'confidence_level': result.confidence.value,
                'is_reliable': result.is_reliable(),
                'encoding': result.encoding,
                'format_features': result.format_features,
                'conversion_suggestions': result.conversion_suggestions
            }
            for file_desc, result in detection_results.items()
        }
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {os.path.basename(report_file)}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” æ•°æ®æ ¼å¼æ£€æµ‹å™¨æ¼”ç¤º")
    print("=" * 80)
    print("æœ¬æ¼”ç¤ºå°†å±•ç¤ºæ•°æ®æ ¼å¼æ£€æµ‹å™¨çš„å®Œæ•´åŠŸèƒ½:")
    print("â€¢ å¤šç§æ•°æ®æ ¼å¼çš„è‡ªåŠ¨è¯†åˆ«")
    print("â€¢ æ ¼å¼æ£€æµ‹ç½®ä¿¡åº¦è¯„ä¼°")
    print("â€¢ æ‰¹é‡æ–‡ä»¶æ ¼å¼æ£€æµ‹")
    print("â€¢ è‡ªåŠ¨æ ¼å¼è½¬æ¢")
    print("â€¢ ä»æ•°æ®å†…å®¹æ£€æµ‹æ ¼å¼")
    print("â€¢ æ”¯æŒçš„æ ¼å¼å’Œè½¬æ¢è·¯å¾„")
    print("=" * 80)
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•å’Œç¤ºä¾‹æ–‡ä»¶
    temp_dir = tempfile.mkdtemp()
    
    try:
        # åˆå§‹åŒ–æ£€æµ‹å™¨
        print("\nğŸš€ åˆå§‹åŒ–æ•°æ®æ ¼å¼æ£€æµ‹å™¨...")
        detector = DataFormatDetector()
        print("âœ… æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶
        print("\nğŸ“ åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶...")
        sample_files = create_sample_data_files(temp_dir)
        print(f"âœ… åˆ›å»ºäº† {len(sample_files)} ä¸ªç¤ºä¾‹æ–‡ä»¶")
        
        # æ¼”ç¤ºæ ¼å¼æ£€æµ‹
        detection_results = demonstrate_format_detection(detector, sample_files)
        
        # æ¼”ç¤ºæ‰¹é‡æ£€æµ‹
        demonstrate_batch_detection(detector, sample_files)
        
        # æ¼”ç¤ºæ ¼å¼è½¬æ¢
        demonstrate_format_conversion(detector, sample_files, temp_dir)
        
        # æ¼”ç¤ºæ•°æ®å†…å®¹æ£€æµ‹
        demonstrate_data_content_detection(detector)
        
        # æ¼”ç¤ºæ”¯æŒçš„æ ¼å¼
        demonstrate_supported_formats(detector)
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        generate_summary_report(detection_results, temp_dir)
        
        print("\n" + "=" * 80)
        print("ğŸ‰ æ•°æ®æ ¼å¼æ£€æµ‹å™¨æ¼”ç¤ºå®Œæˆ!")
        print("=" * 80)
        
        print("\nğŸ“‹ åŠŸèƒ½æ€»ç»“:")
        print("â€¢ âœ… å¤šæ ¼å¼è¯†åˆ« - æ”¯æŒCSVã€JSONã€XMLã€Excelã€æ–‡æœ¬ç­‰æ ¼å¼")
        print("â€¢ âœ… æ™ºèƒ½æ£€æµ‹ - åŸºäºæ–‡ä»¶æ‰©å±•åã€MIMEç±»å‹ã€å†…å®¹ç‰¹å¾çš„ç»¼åˆåˆ¤æ–­")
        print("â€¢ âœ… ç½®ä¿¡åº¦è¯„ä¼° - æä¾›æ£€æµ‹ç»“æœçš„å¯é æ€§è¯„ä¼°")
        print("â€¢ âœ… æ‰¹é‡å¤„ç† - æ”¯æŒæ‰¹é‡æ–‡ä»¶æ ¼å¼æ£€æµ‹")
        print("â€¢ âœ… æ ¼å¼è½¬æ¢ - æ”¯æŒå¤šç§æ ¼å¼é—´çš„è‡ªåŠ¨è½¬æ¢")
        print("â€¢ âœ… å†…å®¹æ£€æµ‹ - å¯ä»æ•°æ®å†…å®¹ç›´æ¥æ£€æµ‹æ ¼å¼")
        print("â€¢ âœ… é”™è¯¯å¤„ç† - å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æŠ¥å‘Š")
        
        print(f"\nğŸ“ ä¸´æ—¶æ–‡ä»¶ç›®å½•: {temp_dir}")
        print("ğŸ’¡ æç¤º: æ¼”ç¤ºç»“æŸåä¸´æ—¶æ–‡ä»¶å°†è¢«è‡ªåŠ¨æ¸…ç†")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            shutil.rmtree(temp_dir)
            print(f"\nğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
        except Exception as e:
            print(f"\nâš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    main()